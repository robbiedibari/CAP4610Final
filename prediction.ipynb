{
 "cells": [
  {
   "cell_type": "code",
   "id": "285c33da-91c6-46d0-b087-7014288bd204",
   "metadata": {},
   "source": [
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.ensemble import RandomForestClassifier, GradientBoostingClassifier\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from sklearn.model_selection import cross_val_score\n",
    "from sklearn.model_selection import learning_curve\n",
    "import numpy as np\n",
    "from sklearn.metrics import confusion_matrix\n",
    "from sklearn.metrics import roc_curve, roc_auc_score"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "source": [
    "'''\n",
    "INTRODUCTION: \n",
    "In the domain of sports analytics, predicting the outcomes of football matches is a challenging yet valuable task. It enables clubs to anticipate game results, assists bettors in decision-making, and enriches fan experiences. This report details the development of  machine learning model designed to predict the results of Premier League football matches offering insights into team performance and match dynamics \n",
    "\n",
    "METHODOLOGY:\n",
    "Our dataset comprises match statistics from the Premier League spanning from 2000 to 2022 seasons. We performed some data cleaning, handled missing values and encoded categorical variables. We engineered features like team point differences and goal statistics. We Evaluated logistic regression, Random Forest, Gradient Boosting models, optimizing Hyperparameter using Grid Search\n",
    "\n",
    "\n",
    "RESULTS: \n",
    "The Gradient Boosting Model achieve the highest accuracy of 69.00% on the testing set. with a precision of  Precision: 67.49 %,  Recall: 81.47% and an F1 Score of 73.82 % . Cross validation yielded a mean accuracy of 63.44% with a standard deviation of 1.04 suggesting stable performance across folds. \n",
    "\n",
    "\n",
    "DISCUSSION: \n",
    "The Gradient Boosting model demonstrated moderate predictive accuracy, with room for improvement. The learning curve indicated that additional training data could further refine the model. Feature importance analysis highlighted 'DiffPts' as the most significant predictor, suggesting that relative team strength is a critical factor in match outcomes. Limitations include the model's sensitivity to feature selection and potential overfitting, as indicated by the disparity between training and validation performance\n",
    "\n",
    "\n",
    "CONCLUSION: \n",
    "Our analysis confirms the potential of machine learning in predicting football match outcomes. While the current model provides a solid foundation, future work could explore more complex features, alternative modeling techniques like deep learning, and real-time data incorporation to enhance prediction accuracy. Continuing to refine the model could yield a valuable tool for strategists and enthusiasts alike\n",
    "'''"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "a14a9d2eca066611",
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "id": "f9f9befd-8fb5-4335-85b3-727d53a68929",
   "metadata": {},
   "source": [
    "matches = pd.read_csv(\"matches.csv\", index_col=0)"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "id": "3fb12d0b-9a2a-4a1c-9449-a4bf7cfea97f",
   "metadata": {},
   "source": [
    "matches.head()"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "id": "b83893da-41a9-4c9f-9be5-16eff1585f4b",
   "metadata": {},
   "source": [
    "matches.shape"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "id": "d56c7e95-01fc-4b43-894f-0ff6339245e5",
   "metadata": {},
   "source": [
    "# 2 seasons * 20 squads * 38 matches\n",
    "2 * 20 * 38"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "id": "7d9f9409-2e0e-4bb5-be86-f490156e2545",
   "metadata": {},
   "source": [
    "# Missing Liverpool 2021-2022\n",
    "matches[\"team\"].value_counts()\n",
    "#3 teams get relegated every year \n",
    "#3 teams get promoted every year into premier league              "
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "id": "564c7aea-a5a0-4d7d-ad1e-5d62fd367f5f",
   "metadata": {},
   "source": [
    "# Realized there is a mission season for liverpool \n",
    "matches[matches[\"team\"] == \"Liverpool\"].sort_values(\"date\")"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "id": "9b7313fe-d6f3-4eae-80c4-93fdd0e8fe59",
   "metadata": {},
   "source": [
    "matches[\"round\"].value_counts()"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "id": "d8c31a87-88c1-47d5-ae27-d3ad690ca2b8",
   "metadata": {},
   "source": [
    "matches.dtypes"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "id": "e5bf7c4f-fd2d-4901-b5dd-36c9ecefccc9",
   "metadata": {},
   "source": [
    "# Cleaning column \"comp\"\n",
    "del matches[\"comp\"]"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "id": "a677a00a-1c26-40bb-ba24-aeb80b3525fe",
   "metadata": {},
   "source": [
    "#Cleaning column \"notes\"\n",
    "del matches[\"notes\"]"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "id": "7120558d-5461-4f32-969c-e88817487bf5",
   "metadata": {},
   "source": [
    "# Converting date to data type in panda. \n",
    "matches[\"date\"] = pd.to_datetime(matches[\"date\"])"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "id": "49cd6dfe-5e67-4253-9e6c-f4d6e2fbb918",
   "metadata": {},
   "source": [
    "# if result = true = winning if not lost or draw \n",
    "matches[\"target\"] = (matches[\"result\"] == \"W\").astype(\"int\")"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "id": "57cca561-db98-4146-bac2-afc6ef86fdfb",
   "metadata": {},
   "source": [
    "matches"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "id": "6a4155d5-2771-4909-801d-1c758e0324a1",
   "metadata": {},
   "source": [
    "# Predictor converting Home/ Away into a numeric column 0=Home 1=Away \n",
    "matches[\"venue_code\"] = matches[\"venue\"].astype(\"category\").cat.codes"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "id": "8f5f47db-5f35-4a11-a59d-f7c8194ec4cb",
   "metadata": {},
   "source": [
    "# Unique code for each opponent team\n",
    "matches[\"opp_code\"] = matches[\"opponent\"].astype(\"category\").cat.codes"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "id": "2198ffa7-0e06-4430-b988-3166e33b65f4",
   "metadata": {},
   "source": [
    "# Maybe some time plays at better time of the day\n",
    "matches[\"hour\"] = matches[\"time\"].str.replace(\":.+\", \"\", regex=True).astype(\"int\")"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "id": "b44fea10-6753-46c9-90bb-76aee9111607",
   "metadata": {},
   "source": [
    "# Day code for each day of the week \n",
    "matches[\"day_code\"] = matches[\"date\"].dt.dayofweek"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "id": "25636d68-933c-497b-a1e0-56dc5c3bc7ac",
   "metadata": {},
   "source": [
    "matches"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "id": "28b49267-efa0-4ba9-9042-f7dd5f17de4e",
   "metadata": {},
   "source": [
    "#Import randomforest classifier \n",
    "# picking up non linear data in the code. \n",
    "from sklearn.ensemble import RandomForestClassifier"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "id": "40bc2deb-6de1-450c-abf0-4ecae2c5ca27",
   "metadata": {},
   "source": [
    "rf = RandomForestClassifier(n_estimators=50, min_samples_split=10, random_state=1)"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "id": "4f4bd9f0-1fe1-4b51-8179-1733e9542305",
   "metadata": {},
   "source": [
    "# Making sure that all the test data come after the data of the training data\n",
    "# taking all the matches before 2022\n",
    "train = matches[matches[\"date\"] < '2022-01-01']"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "id": "dff959c3-646e-4aa8-93ea-ac32dc0f86bb",
   "metadata": {},
   "source": [
    "# Anything in 2022\n",
    "test = matches[matches[\"date\"] > '2022-01-01']"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "id": "6f6ea030-17c1-4658-a6b1-d2e4dcdc2372",
   "metadata": {},
   "source": [
    "# List of features columns we created.\n",
    "predictors = [\"venue_code\", \"opp_code\", \"hour\", \"day_code\"]"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "id": "b1f965d9-1198-4108-80c7-354e5e22652a",
   "metadata": {},
   "source": [
    "rf.fit(train[predictors], train[\"target\"])"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "id": "69a7e5d4-e29c-49c1-9fe9-d2b9becc4335",
   "metadata": {},
   "source": [
    "preds = rf.predict(test[predictors])"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "source": [],
   "metadata": {
    "collapsed": false
   },
   "id": "860b94387ea83549"
  },
  {
   "cell_type": "code",
   "id": "8828b502-d563-43cd-a406-af40495c564d",
   "metadata": {},
   "source": [
    "from sklearn.metrics import accuracy_score"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "id": "d407bc8e-05fc-4bbc-a2c2-9441e5f73f4f",
   "metadata": {},
   "source": [
    "# Determine the accuracy of the model. \n",
    "accuracy = accuracy_score(test[\"target\"], preds)"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "id": "204a3470-e5d5-4401-a936-30f02e9326d4",
   "metadata": {},
   "source": [
    "accuracy"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "id": "2972bf3d-cda6-4d27-a765-f027d7657710",
   "metadata": {},
   "source": [
    "# Combine our actual values vs predicted value \n",
    "combined = pd.DataFrame(dict(actual=test[\"target\"], predicted=preds))"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "id": "c1fdd697-4c7e-46e7-a4ea-1614b75ccd66",
   "metadata": {},
   "source": [
    "# Create when we predicted a 0 and a 1 and what actually happened.\n",
    "pd.crosstab(index=combined[\"actual\"], columns=combined[\"predicted\"])"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "source": [
    "'''\n",
    "0 = predicted a loss or draw \n",
    "1 = Win \n",
    "'''"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "86d27d27bf4976a1",
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "id": "f38e35a0-f17a-401a-8ccc-e83837498227",
   "metadata": {},
   "source": [
    "from sklearn.metrics import precision_score\n",
    "# WHen we predicted a win what percentage of time did the team actually win \n",
    "precision_score(test[\"target\"], preds)"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "id": "19e6953b-3662-4de1-b8c4-fba2bcfabae1",
   "metadata": {},
   "source": [
    "# Create one dataframe for every team \n",
    "grouped_matches = matches.groupby(\"team\")"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "id": "d32bede5-53fd-449b-9615-852bbb7bbe19",
   "metadata": {},
   "source": [
    "group = grouped_matches.get_group(\"Manchester City\").sort_values(\"date\")"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "id": "574fb518-a733-4c35-854b-191cefe1762b",
   "metadata": {},
   "source": [
    "# Take a group in, it will take a set of columns to compute the rolling average and take a new set of new columns to assign rolling averages \n",
    "def rolling_averages(group, cols, new_cols):\n",
    "    # Sort it in order of date\n",
    "    group = group.sort_values(\"date\")\n",
    "    # take a set of columns and computing the rolling averages  closed=\"left\"\n",
    "    rolling_stats = group[cols].rolling(3, closed='left').mean()\n",
    "    group[new_cols] = rolling_stats\n",
    "    # Dropping any missing values \n",
    "    group = group.dropna(subset=new_cols)\n",
    "    return group"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "id": "0ffacb6c-3a8d-4e5c-a2ec-066dcf913cb2",
   "metadata": {},
   "source": [
    "# Columns we want to computer rolling average\n",
    "# gf = goals for, ga= goal against, sh = shots taken, sot = shots on target, dist = distance shot travel, fk = freekick, pk = penalty kicks, pkatt= penalty kicks attends\n",
    "cols = [\"gf\", \"ga\", \"sh\", \"sot\", \"dist\", \"fk\", \"pk\", \"pkatt\"]\n",
    "new_cols = [f\"{c}_rolling\" for c in cols]\n",
    "\n",
    "rolling_averages(group, cols, new_cols)"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "id": "c988a520-ec3b-46c2-8e1e-7981eba5c1e1",
   "metadata": {},
   "source": [
    "matches_rolling = matches.groupby(\"team\").apply(lambda x: rolling_averages(x, cols, new_cols))"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "id": "e336c816-c229-41f7-af68-8e7275907952",
   "metadata": {},
   "source": [
    "matches_rolling"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "id": "156979b1-ad35-4db7-987c-af4910f3c580",
   "metadata": {},
   "source": [
    "matches_rolling = matches_rolling.droplevel('team')"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "id": "e24a3592-9333-4fb6-b194-2a9eb3e576bf",
   "metadata": {},
   "source": [
    "matches_rolling"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "id": "248936e0-febc-422f-a532-7a9827bb5627",
   "metadata": {},
   "source": [
    "# assign value from 0 to 1316 to get unique values for each index. \n",
    "matches_rolling.index = range(matches_rolling.shape[0])"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "id": "6342b84c-f942-4151-9dea-eb5d607a7c33",
   "metadata": {},
   "source": [
    "# Applying changes for better accuracy score\n",
    "def make_predictions(data, predictors):\n",
    "    train = data[data[\"date\"] < '2022-01-01']\n",
    "    test = data[data[\"date\"] > '2022-01-01']\n",
    "    rf.fit(train[predictors], train[\"target\"])\n",
    "    preds = rf.predict(test[predictors])\n",
    "    combined = pd.DataFrame(dict(actual=test[\"target\"], predicted=preds), index=test.index)\n",
    "    error = precision_score(test[\"target\"], preds)\n",
    "    return combined, error"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "id": "baf00a6c-e8e3-4c87-8e32-6aaa6d4205e6",
   "metadata": {},
   "source": [
    "combined, error = make_predictions(matches_rolling, predictors + new_cols)"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "id": "1cef91a4-1111-4fff-b08d-2c6c68f555f1",
   "metadata": {},
   "source": [
    "# 47 to 62\n",
    "error"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "id": "2a5e5ae2-5200-46ef-8883-31253eb2b70e",
   "metadata": {},
   "source": [
    "combined = combined.merge(matches_rolling[[\"date\", \"team\", \"opponent\", \"result\"]], left_index=True, right_index=True)"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "id": "1621248c-4d89-4fba-8bcd-2138b5963a16",
   "metadata": {},
   "source": [
    "combined.head(10)"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "id": "6311715e-2523-4732-becb-731a9dd664b2",
   "metadata": {},
   "source": [
    "class MissingDict(dict):\n",
    "    # Create a class that inherit from the dict class \n",
    "    # panda will not handle any missing keys, it will just remove missing name. and replace it with the value below.  \n",
    "    __missing__ = lambda self, key: key\n",
    "\n",
    "map_values = {\"Brighton and Hove Albion\": \"Brighton\", \n",
    "              \"Manchester United\": \"Manchester Utd\", \n",
    "              \"Newcastle United\": \"Newcastle Utd\", \n",
    "              \"Tottenham Hotspur\": \"Tottenham\", \n",
    "              \"West Ham United\": \"West Ham\", \n",
    "              \"Wolverhampton Wanderers\": \"Wolves\"\n",
    "              } \n",
    "mapping = MissingDict(**map_values)"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "id": "544d6d13-6ac5-42ea-b5ba-d43b53d2daa4",
   "metadata": {},
   "source": [
    "combined[\"new_team\"] = combined[\"team\"].map(mapping)"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "id": "42d5bd18-696d-4d1d-9522-ef3ae44d0324",
   "metadata": {},
   "source": [
    "merged = combined.merge(combined, left_on=[\"date\", \"new_team\"], right_on=[\"date\", \"opponent\"])"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "id": "d4235824-e948-4815-ae0c-75d682e9c10a",
   "metadata": {},
   "source": [
    "merged"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "source": [
    "# Importing a richer data set with 22 season 2000-2022\n",
    "new_data_set = pd.read_csv(\"final_dataset.csv\")"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "64796b2acbf16aaf",
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "source": [
    "print(f\"Number of rows: {new_data_set.shape[0]}\")\n",
    "print(f\"Number of cols: {new_data_set.shape[1]}\")"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "6496ce0a223631de",
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "source": [
    "summary_statistics = new_data_set.describe()"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "b6046e9cbbe3b270",
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "source": [
    "missing_values = new_data_set.isnull().sum()"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "dfb0174be9aa5b99",
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "source": [
    "new_data_set.info()"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "de019aa9c2057f6c",
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "source": [
    "new_data_set.isnull().sum()"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "7e3309e78d1995ae",
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "source": [
    "# Check for duplicates \n",
    "new_data_set.duplicated().sum()"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "6e09ecc58691070",
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "source": [
    "# Dro rows with missing values \n",
    "new_data_set = new_data_set.dropna()"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "58bad0f5b037abd",
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "source": [
    "# Setting up visualization parameters: \n",
    "sns.set(style=\"whitegrid\")\n",
    "# Prepare a figure for multiple plots: \n",
    "fig, axes = plt.subplots(nrows=3, ncols=2, figsize=(14, 18))\n",
    "# Plotting distributions and relationships\n",
    "sns.histplot(data=new_data_set, x='FTHG', bins=10, kde=True, ax=axes[0, 0])\n",
    "axes[0, 0].set_title('Distribution of Full-time Home Goals')\n",
    "\n",
    "sns.histplot(data=new_data_set, x='FTAG', bins=10, kde=True, ax=axes[0, 1])\n",
    "axes[0, 1].set_title('Distribution of Full-time Away Goals')\n",
    "\n",
    "sns.boxplot(x='FTR', y='HTP', data=new_data_set, ax=axes[1, 0])\n",
    "axes[1, 0].set_title('Home Team Points vs Match Result')\n",
    "\n",
    "sns.boxplot(x='FTR', y='ATP', data=new_data_set, ax=axes[1, 1])\n",
    "axes[1, 1].set_title('Away Team Points vs Match Result')\n",
    "\n",
    "sns.scatterplot(x='HTGD', y='ATGD', hue='FTR', data=new_data_set, ax=axes[2, 0])\n",
    "axes[2, 0].set_title('Home vs. Away Team Goal Difference')\n",
    "\n",
    "\n",
    "plt.delaxes(axes[2, 1])\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "e9771b6d32691296",
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "source": [
    "# Feature engineering: Creating interaction features \n",
    "new_data_set['H_vs_A_Goal_Scored_Diff'] = new_data_set['HTGS'] - new_data_set['ATGS']\n",
    "new_data_set['H_vs_A_Goal_Conceded_Diff'] = new_data_set['HTGC'] - new_data_set['ATGC']\n",
    "new_data_set['Points_Ratio'] = new_data_set['HTP'] / (new_data_set['ATP'] + 0.01)  # Adding a small constant to avoid division by zero"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "d3217d8387e10827",
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "source": [
    "# Initialize the LabelEncoder\n",
    "label_encoder = LabelEncoder()\n",
    "    # Encode the 'FTR' column where:\n",
    "# 'A' (Away win) is encoded as 0, 'D' (Draw) as 1, 'H' (Home win) as 2\n",
    "new_data_set['FTR_encoded'] = label_encoder.fit_transform(new_data_set['FTR'])\n",
    "# Display the first few rows to verify the encoding\n",
    "new_data_set[['FTR', 'FTR_encoded']].head()\n"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "b341974d41fb6e22",
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "source": [
    "# Normalization: Scaling goal differences and total goals\n",
    "scaler = MinMaxScaler()\n",
    "new_data_set[['HTGD', 'ATGD', 'H_vs_A_Goal_Scored_Diff', 'H_vs_A_Goal_Conceded_Diff']] = scaler.fit_transform(\n",
    "   new_data_set[['HTGD', 'ATGD', 'H_vs_A_Goal_Scored_Diff', 'H_vs_A_Goal_Conceded_Diff']])"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "96c0495010966669",
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "source": [
    "# Verifing the newly added Features \n",
    "new_data_set[['HTGD', 'ATGD', 'H_vs_A_Goal_Scored_Diff', 'H_vs_A_Goal_Conceded_Diff', 'Points_Ratio']].head()"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "2e3ba6a650e0fd62",
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "source": [
    "print(new_data_set.columns)"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "b79695a9b783c9ea",
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "source": [
    "# Preparing features and target variable\n",
    "features = ['HTGS', 'ATGS', 'HTGC', 'ATGC', 'HTP', 'ATP', 'HTGD', 'ATGD', 'DiffPts', 'DiffFormPts',\n",
    "            'HTFormPts', 'ATFormPts', 'H_vs_A_Goal_Scored_Diff', 'H_vs_A_Goal_Conceded_Diff', 'Points_Ratio']\n",
    "X = new_data_set[features]\n",
    "y = new_data_set['FTR_encoded']"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "1cd2981a4c434185",
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "source": [
    "# Splitting the data into training and testing sets: \n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "a3d0ede97867f0af",
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "source": [
    "# Initialize the models: \n",
    "logistic_model = LogisticRegression(max_iter=1000)\n",
    "random_forest_model = RandomForestClassifier(n_estimators=100)\n",
    "gradient_boosting_model = GradientBoostingClassifier(n_estimators=100)"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "5b70d50d6c93e7b9",
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "source": [
    "# Dictionary to Hold models and their performances \n",
    "model_performance = {}\n",
    "\n",
    "def train_evaluate_model(model, model_name):\n",
    "    model.fit(X_train, y_train)\n",
    "    y_pred = model.predict(X_train)\n",
    "    accuracy = accuracy_score(y_train, y_pred)\n",
    "    precision = precision_score(y_train, y_pred, average='weighted')\n",
    "    recall = recall_score(y_train, y_pred, average='weighted')\n",
    "    f1 = f1_score(y_train, y_pred, average='weighted')\n",
    "    \n",
    "    # Storing performance metrics\n",
    "    model_performance[model_name] = {\n",
    "        'Accuracy': accuracy,\n",
    "        'Precision': precision,\n",
    "        'Recall': recall,\n",
    "        'F1 Score': f1\n",
    "    }\n",
    "\n",
    "# Train and evaluate each model\n",
    "print(\"Training the model...\")\n",
    "train_evaluate_model(logistic_model, \"Logistic Regression\")\n",
    "train_evaluate_model(random_forest_model, \"Random Forest\")\n",
    "train_evaluate_model(gradient_boosting_model, \"Gradient Boosting\")\n",
    "\n",
    "model_performance"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "31105f58c0d47cf3",
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "source": [
    "def test_evaluate_model(model, model_name):\n",
    "    model.fit(X_train, y_train)\n",
    "    y_pred = model.predict(X_test)\n",
    "    accuracy = accuracy_score(y_test, y_pred)\n",
    "    precision = precision_score(y_test, y_pred, average='weighted')\n",
    "    recall = recall_score(y_test, y_pred, average='weighted')\n",
    "    f1 = f1_score(y_test, y_pred, average='weighted')\n",
    "    \n",
    "    # Storing performance metrics\n",
    "    model_performance[model_name] = {\n",
    "        'Accuracy': accuracy,\n",
    "        'Precision': precision,\n",
    "        'Recall': recall,\n",
    "        'F1 Score': f1\n",
    "    }\n",
    "\n",
    "# Train and evaluate each model\n",
    "print(\"Testing model \")\n",
    "test_evaluate_model(logistic_model, \"Logistic Regression\")\n",
    "test_evaluate_model(random_forest_model, \"Random Forest\")\n",
    "test_evaluate_model(gradient_boosting_model, \"Gradient Boosting\")\n",
    "\n",
    "model_performance"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "59a249f65e555ed8",
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "source": [
    "# Gradient Boosting was the model the performed better, so we went ahead and started the Hyperparameter tuning to improve the model performance \n",
    "from sklearn.model_selection import GridSearchCV\n",
    "\n",
    "# Set up the parameter grid for Gradient Boosting\n",
    "param_grid = {\n",
    "    'n_estimators': [50, 100, 150],\n",
    "    'learning_rate': [0.05, 0.1, 0.2],\n",
    "    'max_depth': [3, 5, 7],\n",
    "    'min_samples_split': [2, 4],\n",
    "    'min_samples_leaf': [1, 2]\n",
    "}\n",
    "\n",
    "# Initialize the GridSearchCV object\n",
    "grid_search = GridSearchCV(estimator=GradientBoostingClassifier(random_state=42),\n",
    "                           param_grid=param_grid,\n",
    "                           scoring='accuracy',  # focusing on accuracy as the metric for simplicity\n",
    "                           cv=3,  # using 3-fold cross-validation\n",
    "                           verbose=1,  # printing out progress\n",
    "                           n_jobs=-1)  # using all available CPUs\n",
    "\n",
    "# Fit GridSearchCV to the training data\n",
    "grid_search.fit(X_train, y_train)\n",
    "\n",
    "# Best parameters and best score\n",
    "best_params = grid_search.best_params_\n",
    "best_score = grid_search.best_score_\n",
    "\n",
    "best_params, best_score\n"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "7323bb4128120ddf",
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "source": [
    "# Re-training the Gradient Boosting model with the optimized parameters\n",
    "optimized_gb_model = GradientBoostingClassifier(\n",
    "    n_estimators=50,\n",
    "    learning_rate=0.05,\n",
    "    max_depth=3,\n",
    "    min_samples_split=2,\n",
    "    min_samples_leaf=2,\n",
    "    random_state=42\n",
    ")\n",
    "\n",
    "# Fit the model to the training data\n",
    "optimized_gb_model.fit(X_train, y_train)\n",
    "\n",
    "# Predict on the training data\n",
    "y_train_pred_optimized = optimized_gb_model.predict(X_train)\n",
    "\n",
    "# Calculate the accuracy on the testing data\n",
    "optimized_training_accuracy = accuracy_score(y_train, y_train_pred_optimized)\n",
    "\n",
    "optimized_training_accuracy, "
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "7c2b3316cbef688d",
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "source": [
    "# Re-Testing the Gradient Boosting Model with the optimized parameters \n",
    "optimized_gb_model_test = GradientBoostingClassifier(\n",
    "    n_estimators=50,\n",
    "    learning_rate=0.05,\n",
    "    max_depth=3,\n",
    "    min_samples_split=2,\n",
    "    min_samples_leaf=2,\n",
    "    random_state=42\n",
    ")\n",
    "# Fit the model to the test data \n",
    "optimized_gb_model_test.fit(X_test, y_test)\n",
    "# Predicting on the testing data \n",
    "y_test_pred_optimized = optimized_gb_model_test.predict(X_test)\n",
    "\n",
    "# Calculating the accuracy: \n",
    "optimized_testing_accuracy = accuracy_score(y_test, y_test_pred_optimized)\n",
    "\n",
    "# Calculate Precision\n",
    "optimized_precision = precision_score(y_test, y_test_pred_optimized)\n",
    "\n",
    "# Calculate Recall\n",
    "optimized_recall = recall_score(y_test, y_test_pred_optimized)\n",
    "\n",
    "# Calculate F1 Score\n",
    "optimized_f1 = f1_score(y_test, y_test_pred_optimized)\n",
    "\n",
    "\n",
    "print(\"Accuracy: \",optimized_testing_accuracy)\n",
    "\n",
    "print(\"Precision:\", optimized_precision)\n",
    "print(\"Recall:\", optimized_recall)\n",
    "print(\"F1 Score:\", optimized_f1)\n"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "b0bc85d589c6e789",
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "source": [
    "# Assuming 'optimized_gb_model' is your trained Gradient Boosting model and 'X' is your feature matrix\n",
    "feature_importances = optimized_gb_model.feature_importances_\n",
    "\n",
    "# Creating a DataFrame to hold the feature names and their corresponding importance\n",
    "features_df = pd.DataFrame({\n",
    "    'Feature': X.columns,\n",
    "    'Importance': feature_importances\n",
    "})\n",
    "\n",
    "# Sorting the DataFrame by the 'Importance' column in descending order\n",
    "features_df = features_df.sort_values(by='Importance', ascending=False)\n",
    "\n",
    "# Plotting the feature importances\n",
    "plt.figure(figsize=(10, 8))\n",
    "sns.barplot(x='Importance', y='Feature', data=features_df)\n",
    "plt.title('Feature Importance in Optimized Gradient Boosting Model')\n",
    "plt.xlabel('Importance')\n",
    "plt.ylabel('Feature')\n",
    "plt.show()\n",
    "\n",
    "# Print the DataFrame to view it in tabular form\n",
    "print(features_df)\n"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "3e77c69fb5847af3",
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "source": [
    "'''    DiffPts: Likely represents the difference in points between the two teams. Points could be the total accumulated over the season or a set of games.\n",
    "    H_vs_A_Goal_Scored_Diff: Home vs Away Goal Scored Diff\n",
    "    ATP:  \"Away Team Points,\" \n",
    "    HTGD: \"Home Team Goal Difference\" \n",
    "    ATGD: \"Away Team Goal Difference\" \n",
    "    Points_Ratio: Likely the ratio of points between the home and away teams, providing a relative measure of their strengths.\n",
    "    HTP: \"Home Team Points\" \n",
    "    H_vs_A_Goal_Conceded_Diff: \n",
    "    DiffFormPts: Likely represents the difference in form points between the two teams, which could be based on recent performances.\n",
    "    ATFormPts: \"Away Team Form Points\"\n",
    "    ATGC: \"Away Team Goals Conceded\" \n",
    "    HTGS: \"Home Team Goals Scored\"\n",
    "    HTGC: \"Home Team Goals Conceded\" \n",
    "    HTFormPts: \"Home Team Form Points\"\n",
    "    ATGS: \"Away Team Goals Scored\" \n",
    "    '''"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "200978c6194db527",
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "source": [
    "# Validation: \n",
    "# Using 5-fold cross-validation to validate the optimized Gradient Boosting model\n",
    "cv_scores = cross_val_score(optimized_gb_model, X, y, cv=5, scoring='accuracy')\n",
    "\n",
    "# Calculate the mean and standard deviation of the cross-validation scores\n",
    "cv_mean = cv_scores.mean()\n",
    "cv_std = cv_scores.std()\n",
    "\n",
    "# Gives you an average score across all fold providing an insight into how generally well the model performs\n",
    "print(\"Cross-Validation Mean Accuracy: {:.2f}%\".format(cv_mean * 100))\n",
    "# Indicates the varaibility of the model's performance, giving you an idea about its stabiliy across different subsets \n",
    "print(\"Cross-Validation Standard Deviation: {:.2f}%\".format(cv_std * 100))"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "ca881e8658eb73ef",
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "source": [
    "from sklearn.model_selection import learning_curve\n",
    "\n",
    "# Define function to plot learning curves\n",
    "def plot_learning_curve(estimator, title, X, y, ylim=None, cv=None, n_jobs=None, train_sizes=np.linspace(.1, 1.0, 5)):\n",
    "    plt.figure()\n",
    "    plt.title(title)\n",
    "    if ylim is not None:\n",
    "        plt.ylim(*ylim)\n",
    "    plt.xlabel(\"Training examples\")\n",
    "    plt.ylabel(\"Score\")\n",
    "    train_sizes, train_scores, test_scores = learning_curve(\n",
    "        estimator, X, y, cv=cv, n_jobs=n_jobs, train_sizes=train_sizes)\n",
    "    train_scores_mean = np.mean(train_scores, axis=1)\n",
    "    train_scores_std = np.std(train_scores, axis=1)\n",
    "    test_scores_mean = np.mean(test_scores, axis=1)\n",
    "    test_scores_std = np.std(test_scores, axis=1)\n",
    "    plt.grid()\n",
    "\n",
    "    plt.fill_between(train_sizes, train_scores_mean - train_scores_std,\n",
    "                     train_scores_mean + train_scores_std, alpha=0.1, color=\"r\")\n",
    "    plt.fill_between(train_sizes, test_scores_mean - test_scores_std,\n",
    "                     test_scores_mean + test_scores_std, alpha=0.1, color=\"g\")\n",
    "    plt.plot(train_sizes, train_scores_mean, 'o-', color=\"r\",\n",
    "             label=\"Training score\")\n",
    "    plt.plot(train_sizes, test_scores_mean, 'o-', color=\"g\",\n",
    "             label=\"Cross-validation score\")\n",
    "\n",
    "    plt.legend(loc=\"best\")\n",
    "    return plt\n",
    "\n",
    "# Generate the learning curve for the optimized Gradient Boosting model\n",
    "cv = 5  # 5-fold cross-validation\n",
    "plot_learning_curve(optimized_gb_model, 'Learning Curve for Gradient Boosting', X, y, cv=cv, n_jobs=-1)\n"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "169e62f52bacccbf",
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "source": [
    "def plot_roc_curve(model, X_test, y_test):\n",
    "    # Predict probabilities for the positive class\n",
    "    probs = model.predict_proba(X_test)[:, 1]\n",
    "    # Calculate ROC curve\n",
    "    fpr, tpr, thresholds = roc_curve(y_test, probs)\n",
    "    # Calculate AUC score\n",
    "    auc = roc_auc_score(y_test, probs)\n",
    "    \n",
    "    # Plot\n",
    "    plt.figure(figsize=(8, 6))\n",
    "    plt.plot(fpr, tpr, color='darkorange', lw=2, label=f'ROC curve (area = {auc:.2f})')\n",
    "    plt.plot([0, 1], [0, 1], color='navy', lw=2, linestyle='--')\n",
    "    plt.xlim([0.0, 1.0])\n",
    "    plt.ylim([0.0, 1.05])\n",
    "    plt.xlabel('False Positive Rate')\n",
    "    plt.ylabel('True Positive Rate')\n",
    "    plt.title('Receiver Operating Characteristic (ROC)')\n",
    "    plt.legend(loc=\"lower right\")\n",
    "    plt.show()\n",
    "\n",
    "# Example of plotting ROC curve for the Gradient Boosting model\n",
    "plot_roc_curve(optimized_gb_model, X_test, y_test)"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "94255aa4b55b5b11",
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "source": [
    "def plot_confusion_matrix(model, X_test, y_test):\n",
    "    # Predictions\n",
    "    y_pred = model.predict(X_test)\n",
    "    # Generate confusion matrix\n",
    "    cm = confusion_matrix(y_test, y_pred)\n",
    "    # Plot\n",
    "    plt.figure(figsize=(8, 6))\n",
    "    sns.heatmap(cm, annot=True, fmt='d')\n",
    "    plt.xlabel('Predicted')\n",
    "    plt.ylabel('True')\n",
    "    plt.title('Confusion Matrix')\n",
    "    plt.show()\n",
    "\n",
    "# Example of plotting Confusion Matrix for the Gradient Boosting model\n",
    "plot_confusion_matrix(optimized_gb_model, X_test, y_test)"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "c3572c13dc72e419",
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "source": [
    "'''\n",
    "Label 0 represent Home loss or Not Win \n",
    "Label 1 represent Home Win \n",
    "\n",
    "Top Left True Negative 328: Represent correct precitions for the negative class. \n",
    "Top-Right False Positive 306: This represents the error where the model incorecctly predicted the positve class. \n",
    "Bottom-Left False Negative 180: This represents the error where the model incorrectly predicted the negative class \n",
    "Bottom Right: True positive 554: This represent the correct predictions for the positive class.\n",
    "'''"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "eed3872f4fb0b50c",
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "source": [
    "# Preparing features and encoding the target 'FTR'\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "\n",
    "# Encoding the target\n",
    "label_encoder = LabelEncoder()\n",
    "new_data_set['FTR_encoded'] = label_encoder.fit_transform(new_data_set['FTR'])\n",
    "\n",
    "# Selecting a subset of features for simplicity\n",
    "features = ['HTGS', 'ATGS', 'HTGC', 'ATGC', 'HTP', 'ATP', 'HTGD', 'ATGD', 'DiffPts', 'DiffFormPts']\n",
    "X = new_data_set[features]\n",
    "y = new_data_set['FTR_encoded']\n",
    "\n",
    "# Splitting the data into training and testing sets\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "\n",
    "# Re-initialize and train the Random Forest and Gradient Boosting models\n",
    "random_forest_model = RandomForestClassifier(n_estimators=100, random_state=42)\n",
    "gradient_boosting_model = GradientBoostingClassifier(n_estimators=50, learning_rate=0.05, max_depth=3, \n",
    "                                                     min_samples_split=2, min_samples_leaf=2, random_state=42)\n",
    "\n",
    "# Train the models\n",
    "random_forest_model.fit(X_train, y_train)\n",
    "gradient_boosting_model.fit(X_train, y_train)\n",
    "\n",
    "# Making predictions on the test set\n",
    "y_pred_rf = random_forest_model.predict(X_test)\n",
    "y_pred_gb = gradient_boosting_model.predict(X_test)\n",
    "\n",
    "# Mapping predictions back to readable format\n",
    "y_pred_rf_mapped = label_encoder.inverse_transform(y_pred_rf)\n",
    "y_pred_gb_mapped = label_encoder.inverse_transform(y_pred_gb)\n",
    "\n",
    "# Extracting actual team names and results from the test set\n",
    "test_team_info = new_data_set.loc[X_test.index, ['HomeTeam', 'AwayTeam', 'FTR']]\n",
    "\n",
    "# Creating the final DataFrame\n",
    "final_predictions_with_actual_names = pd.DataFrame({\n",
    "    'HomeTeam': test_team_info['HomeTeam'],\n",
    "    'AwayTeam': test_team_info['AwayTeam'],\n",
    "    'Actual Result': test_team_info['FTR'],\n",
    "    'RF Prediction': y_pred_rf_mapped,\n",
    "    'GB Prediction': y_pred_gb_mapped\n",
    "})\n",
    "\n",
    "final_predictions_with_actual_names\n"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "8d7c52db58b2c32",
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "source": [
    "'''\n",
    "H = Home win\n",
    "NH = Not Home Win \n",
    "'''"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "91510ce89757d9d2",
   "outputs": [],
   "execution_count": null
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
